{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8710cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patwuch/miniforge3/envs/gee_dengue_ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using validation strategy: walk in version 0905\n",
      "Land Use Features Included: False\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;66;03m# fallback\u001b[39;00m\n\u001b[1;32m     90\u001b[0m PROJECT_ROOT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome/patwuch/projects/gee_dengue_ml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 91\u001b[0m RAW_DIR \u001b[38;5;241m=\u001b[39m \u001b[43mPROJECT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m INTERIM_DIR \u001b[38;5;241m=\u001b[39m PROJECT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterim\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m PROCESSED_DIR \u001b[38;5;241m=\u001b[39m PROJECT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import optuna\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import psutil\n",
    "import pynvml\n",
    "import graphviz\n",
    "import shap\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from dtreeviz import model as dtreeviz_model\n",
    "import warnings\n",
    "\n",
    "# Initialize pynvml for GPU memory logging\n",
    "try:\n",
    "    pynvml.nvmlInit()\n",
    "    def log_memory(tag=\"\"):\n",
    "        \"\"\"Log system RAM + GPU VRAM usage.\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        ram_mb = process.memory_info().rss / 1024**2\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_used_mb = gpu_mem.used / 1024**2\n",
    "        print(f\"[{tag}] RAM: {ram_mb:.2f} MB | GPU: {gpu_used_mb:.2f} MB\")\n",
    "except pynvml.NVMLError as e:\n",
    "    print(f\"Warning: pynvml could not be initialized. GPU memory logging disabled. Error: {e}\")\n",
    "    def log_memory(tag=\"\"):\n",
    "        pass # Placeholder for when GPU logging is not possible\n",
    "\n",
    "# --- Configuration ---\n",
    "config_input = \"WF0905\"\n",
    "\n",
    "config_input = config_input.strip().lower()\n",
    "if len(config_input) != 6:\n",
    "    raise ValueError(\"Invalid config format. Use 6 characters like 'kt0726' or 'wf2031'.\")\n",
    "\n",
    "validation_flag = config_input[0]\n",
    "landuse_flag = config_input[1]\n",
    "version_digits = config_input[2:]\n",
    "\n",
    "if validation_flag == 'k':\n",
    "    validation_strategy = 'kfold'\n",
    "elif validation_flag == 'w':\n",
    "    validation_strategy = 'walk'\n",
    "else:\n",
    "    raise ValueError(\"Invalid validation flag. Use 'K' for kfold or 'W' for walk_forward.\")\n",
    "\n",
    "if landuse_flag == 't':\n",
    "    USE_LANDUSE_FEATURES = True\n",
    "    landuse_suffix = \"lulc\"\n",
    "elif landuse_flag == 'f':\n",
    "    USE_LANDUSE_FEATURES = False\n",
    "    landuse_suffix = \"\"\n",
    "else:\n",
    "    raise ValueError(\"Land use flag must be 'T' or 'F'.\")\n",
    "\n",
    "if not version_digits.isdigit():\n",
    "    raise ValueError(\"Version must be 4 digits.\")\n",
    "VERSION_STAMP = version_digits\n",
    "version_suffix = f\"{VERSION_STAMP}\"\n",
    "\n",
    "print(f\"Using validation strategy: {validation_strategy} in version {VERSION_STAMP}\")\n",
    "print(f\"Land Use Features Included: {USE_LANDUSE_FEATURES}\")\n",
    "all_study_name = f\"xgbC-nationwide-{validation_strategy}-{landuse_suffix}-{VERSION_STAMP}\"\n",
    "\n",
    "# --- Paths (adapted for notebook) ---\n",
    "# Assuming the notebook is run from the project root or you manually set the path\n",
    "PROJECT_ROOT = Path(os.getcwd())\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "INTERIM_DIR = PROJECT_ROOT / \"data\" / \"interim\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "EXTERNAL_DIR = PROJECT_ROOT / \"data\" / \"external\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "FIGURES_DIR = REPORTS_DIR / \"figures\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "TABLES_DIR = REPORTS_DIR / \"tables\"\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(PROCESSED_DIR / \"INDONESIA\" / \"monthly_dengue_env_id_class_log.csv\")\n",
    "df['Risk_Category'] = df['Risk_Category'].replace({'Zero': 0, 'Low': 1, 'High': 2}).infer_objects(copy=False)\n",
    "df['Risk_Category'] = df['Risk_Category'].astype('int32')\n",
    "num_classes = df['Risk_Category'].nunique()\n",
    "df['YearMonth'] = pd.to_datetime(df['YearMonth'])\n",
    "df = df.sort_values(['YearMonth', 'ID_2'])\n",
    "\n",
    "# --- Define Features and Target ---\n",
    "env_vars = ['temperature_2m', 'temperature_2m_min', 'temperature_2m_max', 'precipitation', 'potential_evaporation_sum', 'total_evaporation_sum', 'evaporative_stress_index', 'aridity_index', 'temperature_2m_ANOM', 'temperature_2m_min_ANOM', 'temperature_2m_max_ANOM', 'potential_evaporation_sum_ANOM', 'total_evaporation_sum_ANOM', 'precipitation_ANOM']\n",
    "land_use_vars = ['Class_70', 'Class_60', 'Class_50', 'Class_40', 'Class_95', 'Class_30', 'Class_20', 'Class_10', 'Class_90', 'Class_80']\n",
    "climate_vars = ['ANOM1+2', 'ANOM3', 'ANOM4', 'ANOM3.4', 'DMI', 'DMI_East']\n",
    "\n",
    "for var_group in [env_vars, climate_vars]:\n",
    "    for var in var_group:\n",
    "        for lag in [1, 2, 3]:\n",
    "            df[f'{var}_lag{lag}'] = df.groupby('ID_2')[var].shift(lag)\n",
    "\n",
    "variable_columns = env_vars + climate_vars\n",
    "if USE_LANDUSE_FEATURES:\n",
    "    variable_columns += land_use_vars\n",
    "for var_group in [env_vars, climate_vars]:\n",
    "    for var in var_group:\n",
    "        for lag in [1, 2, 3]:\n",
    "            lagged_var = f'{var}_lag{lag}'\n",
    "            if lagged_var in df.columns:\n",
    "                variable_columns.append(lagged_var)\n",
    "\n",
    "target = 'Risk_Category'\n",
    "metadata_columns = ['YearMonth', 'ID_2', 'Region_Group','Incidence_Rate']\n",
    "variable_columns = [col for col in variable_columns if col not in [metadata_columns, target]]\n",
    "\n",
    "df_train_val_national = df[df['YearMonth'].dt.year < 2023].copy().dropna(subset=variable_columns + [target])\n",
    "df_test_national = df[df['YearMonth'].dt.year == 2023].copy().dropna(subset=variable_columns + [target])\n",
    "\n",
    "cudf_train_val_national = cudf.DataFrame(df_train_val_national)[variable_columns + metadata_columns + [target]]\n",
    "cudf_test_national = cudf.DataFrame(df_test_national)[variable_columns + metadata_columns + [target]]\n",
    "\n",
    "print(\"Starting training with the following columns:\")\n",
    "print(\"--- Target Column ---\\n\", [target])\n",
    "print(\"-\" * 50)\n",
    "print(\"--- Metadata Columns ---\\n\", metadata_columns)\n",
    "print(\"-\" * 50)\n",
    "print(\"--- Variable Columns ---\\n\", variable_columns)\n",
    "\n",
    "def calculate_sample_weights(y):\n",
    "    unique_classes, counts = np.unique(y, return_counts=True)\n",
    "    total_samples = len(y)\n",
    "    num_classes = len(unique_classes)\n",
    "    weights = {cls: total_samples / (num_classes * counts[i]) for i, cls in enumerate(unique_classes)}\n",
    "    return np.array([weights[cls] for cls in y])\n",
    "\n",
    "def get_splits_gpu(df, validation_flag, train_window=None, test_window=None):\n",
    "    if validation_flag != 'w':\n",
    "        raise ValueError(f\"Unsupported validation flag: {validation_flag}\")\n",
    "    if test_window is None:\n",
    "        raise ValueError(\"test_window must be provided for walk-forward validation\")\n",
    "\n",
    "    df_sorted = df.sort_values('YearMonth').reset_index(drop=True)\n",
    "    unique_time_steps = df_sorted['YearMonth'].unique()\n",
    "    n_time_steps = len(unique_time_steps)\n",
    "    splits = []\n",
    "    initial_train_window = 36 if train_window is None else train_window\n",
    "    num_folds = (n_time_steps - initial_train_window) // test_window\n",
    "    \n",
    "    for fold_num in range(num_folds):\n",
    "        test_start_idx_time = initial_train_window + fold_num * test_window\n",
    "        test_end_idx_time = test_start_idx_time + test_window\n",
    "        test_start_time = unique_time_steps[test_start_idx_time]\n",
    "        test_end_time = unique_time_steps[test_end_idx_time]\n",
    "        train_start_time = unique_time_steps[0] if train_window is None else unique_time_steps[test_start_idx_time - train_window]\n",
    "        \n",
    "        train_start_row_idx = df_sorted['YearMonth'].searchsorted(train_start_time, side='left')\n",
    "        train_end_row_idx = df_sorted['YearMonth'].searchsorted(test_start_time, side='left')\n",
    "        test_start_row_idx = df_sorted['YearMonth'].searchsorted(test_start_time, side='left')\n",
    "        test_end_row_idx = df_sorted['YearMonth'].searchsorted(test_end_time, side='left')\n",
    "        \n",
    "        train_idx = cudf.RangeIndex(train_start_row_idx, train_end_row_idx)\n",
    "        test_idx = cudf.RangeIndex(test_start_row_idx, test_end_row_idx)\n",
    "        \n",
    "        if not train_idx.empty and not test_idx.empty:\n",
    "            splits.append((fold_num, train_idx, test_idx))\n",
    "    return splits\n",
    "\n",
    "# --- Hyperparameter Tuning (commented out as in original script) ---\n",
    "# def objective(trial, X_gpu, y_gpu, splits, num_classes=2):\n",
    "#     # ... (original objective function code) ...\n",
    "#     return acc\n",
    "\n",
    "# all_best_hypers = []\n",
    "# # ... (rest of the hyperparameter tuning code) ...\n",
    "\n",
    "# --- Begin Final Model Training and Evaluation ---\n",
    "print(\"\\n--- Begin Final Model Training and Evaluation ---\")\n",
    "\n",
    "hyperparams_csv_path = TABLES_DIR / f\"{all_study_name}_params.csv\"\n",
    "try:\n",
    "    hyperparams_df = pd.read_csv(hyperparams_csv_path)\n",
    "    print(f\"Extracted hyperparameters from: {hyperparams_csv_path}\")\n",
    "    nationwide_params_row = hyperparams_df[hyperparams_df['Region'] == 'Nationwide'].iloc[0]\n",
    "    national_hyperparams = {\n",
    "        'gamma': nationwide_params_row['gamma'],\n",
    "        'n_estimators': int(nationwide_params_row['n_estimators']),\n",
    "        'max_depth': int(nationwide_params_row['max_depth']),\n",
    "        'reg_alpha': nationwide_params_row['reg_alpha'],\n",
    "        'subsample': nationwide_params_row['subsample'],\n",
    "        'reg_lambda': nationwide_params_row['reg_lambda'],\n",
    "        'learning_rate': nationwide_params_row['learning_rate'],\n",
    "        'colsample_bytree': nationwide_params_row['colsample_bytree'],\n",
    "        'min_child_weight': int(nationwide_params_row['min_child_weight']),\n",
    "        'device': 'cuda'\n",
    "    }\n",
    "    num_round = int(nationwide_params_row['n_estimators'])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Hyperparameters file not found at {hyperparams_csv_path}. Using default/fallback parameters.\")\n",
    "    national_hyperparams = {\n",
    "        'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7, 'device': 'cuda'\n",
    "    }\n",
    "    num_round = 100\n",
    "\n",
    "if num_classes > 2:\n",
    "    national_hyperparams['objective'] = 'multi:softprob'\n",
    "    national_hyperparams['num_class'] = num_classes\n",
    "    shap_link_func = 'logit'\n",
    "else:\n",
    "    national_hyperparams['objective'] = 'binary:logistic'\n",
    "    shap_link_func = 'logistic'\n",
    "\n",
    "X_train_val = cudf_train_val_national[variable_columns]\n",
    "y_train_val = cudf_train_val_national[[target]].to_pandas().values.flatten()\n",
    "X_test = cudf_test_national[variable_columns]\n",
    "y_test = cudf_test_national[[target]].to_pandas().values.flatten()\n",
    "\n",
    "X_train_pd = X_train_val.to_pandas()\n",
    "X_test_pd = X_test.to_pandas()\n",
    "    \n",
    "sample_weights = calculate_sample_weights(y_train_val)\n",
    "Dtrain = xgboost.DMatrix(X_train_val, label=y_train_val, weight=sample_weights)\n",
    "Dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "\n",
    "model = xgboost.train(national_hyperparams, Dtrain, num_boost_round=num_round)\n",
    "model.set_param({\"device\": \"cuda\"})\n",
    "\n",
    "# --- Visualize Decision Tree in Notebook ---\n",
    "print(\"\\n--- Visualizing a single tree from the model ---\")\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "viz_api = dtreeviz_model(\n",
    "    model,\n",
    "    X_train=X_train_val.to_pandas().values,\n",
    "    y_train=y_train_val,\n",
    "    feature_names=variable_columns,\n",
    "    target_name=target,\n",
    "    class_names=[str(i) for i in range(num_classes)],\n",
    "    tree_index=2\n",
    ")\n",
    "viz_api.view(fontname=\"DejaVu Sans\")\n",
    "print(\"Decision tree visualization displayed.\")\n",
    "\n",
    "# --- Predict and Compute Performance Metrics ---\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "y_pred_prob = model.predict(Dtest)\n",
    "if num_classes > 2:\n",
    "    y_pred = y_pred_prob.argmax(axis=1)\n",
    "else:\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "target_counts = pd.Series(y_test).value_counts().to_dict()\n",
    "\n",
    "print(f\"Results for Nationwide: Accuracy={accuracy:.4f}\")\n",
    "print(f\"Target distribution for Nationwide: {target_counts}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Plot Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, y_pred, labels=list(range(num_classes)))\n",
    "plt.figure(figsize=(num_classes * 2, num_classes * 2))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=list(range(num_classes)), yticklabels=list(range(num_classes)))\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(f\"Confusion Matrix - Nationwide\")\n",
    "plt.show()\n",
    "\n",
    "# --- SHAP Analysis and Plotting ---\n",
    "print(\"\\n--- SHAP Analysis ---\")\n",
    "def get_shap_array(shap_obj):\n",
    "    if isinstance(shap_obj, list):\n",
    "        shap_arrays = [sv.values if hasattr(sv, \"values\") else sv for sv in shap_obj]\n",
    "        if len(shap_arrays) == 2:\n",
    "            return shap_arrays[1]\n",
    "        else:\n",
    "            return np.mean([np.abs(sv) for sv in shap_arrays], axis=0)\n",
    "    else:\n",
    "        return shap_obj.values if hasattr(shap_obj, \"values\") else shap_obj\n",
    "\n",
    "try:\n",
    "    explainer = shap.explainers.GPUTree(model, X_test_pd, feature_perturbation=\"tree_path_dependent\")\n",
    "    shap_values_raw = explainer(X_test_pd)\n",
    "    overall_shap_values_for_importance = get_shap_array(shap_values_raw)\n",
    "    feature_importances = np.mean(np.abs(overall_shap_values_for_importance), axis=0)\n",
    "    top_5_features_idx = np.argsort(feature_importances)[::-1][:5]\n",
    "    top_5_features = [X_test_pd.columns[i] for i in top_5_features_idx]\n",
    "    print(f\"\\nTop 5 predictors for Nationwide: {top_5_features}\")\n",
    "\n",
    "    # Beeswarm plot\n",
    "    print(\"\\n--- Generating SHAP Beeswarm Plot ---\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values_raw, X_test_pd, show=False)\n",
    "    plt.title(f\"SHAP Beeswarm Plot - Nationwide\")\n",
    "    plt.show()\n",
    "\n",
    "    # Dependence plots for top features\n",
    "    print(\"\\n--- Generating SHAP Dependence Plots for Top 5 Features ---\")\n",
    "    for feature in top_5_features:\n",
    "        for class_idx in range(num_classes):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.dependence_plot(\n",
    "                feature, \n",
    "                shap_values_raw[..., class_idx], \n",
    "                X_test_pd, \n",
    "                interaction_index=None, \n",
    "                show=False\n",
    "            )\n",
    "            plt.title(f\"SHAP Dependence Plot for {feature}, Class {class_idx} - Nationwide\")\n",
    "            plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to generate SHAP plots for the nationwide model. Error: {e}\")\n",
    "\n",
    "# Save summary to CSV\n",
    "all_results = [{\"Region\": \"Nationwide\", \"Accuracy\": accuracy, **{f\"Precision_{i}\": report[str(i)]['precision'] for i in range(num_classes) if str(i) in report},\n",
    "                **{f\"Recall_{i}\": report[str(i)]['recall'] for i in range(num_classes) if str(i) in report},\n",
    "                **{f\"F1_{i}\": report[str(i)]['f1-score'] for i in range(num_classes) if str(i) in report}}]\n",
    "summary_df = pd.DataFrame(all_results)\n",
    "csv_filename = TABLES_DIR / f\"{all_study_name}_national_results.csv\"\n",
    "summary_df.to_csv(csv_filename, index=False, float_format=\"%.4f\")\n",
    "print(f\"\\nNational summary table saved to '{csv_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1527a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee_dengue_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
